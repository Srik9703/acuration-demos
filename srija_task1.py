# -*- coding: utf-8 -*-
"""intern_task1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vgaQ1ysZgCy_XkMXuYAHVmkysFCFhues
"""







#extracting urls from sitemap
import requests
from bs4 import BeautifulSoup

def parse_sitemap(sitemap_path):
  try:
    #using get req to get data
    f=requests.get(sitemap_path)
    k=f.content
    #using xml parser to find all links
    soup = BeautifulSoup(k, 'xml')
    urls_list = []
    #getting all the links
    loc_tags =soup.find_all('loc')
    #adding all links into a list
    for i in loc_tags:
          urls_list.append(i.get_text())
    #print("no.of urls=",len(urls_list))
    #for i in urls_list:
    # if "pdf" in i:
    #   print(i)
    return urls_list
  except Exception as e:
        print(f"An error occurred: {e}")
        return None

import re

def clean_html_and_extract_text(html_text):
  try:
    #using a html parser
      soup=BeautifulSoup(html_text,'html.parser',from_encoding="iso-8859-1")

      for data in soup(['style', 'script']):
          # Remove tags
          data.decompose()

      #retriving only inner text of an element
      k=' '.join(soup.stripped_strings)
      #removing all special characters
      without_spcl_char=re.sub(r'[^a-zA-Z0-9\s]+', '', k)
      #removing extra whitespaces
      cleaned_text=" ".join(without_spcl_char.split())

      return cleaned_text.lower()
  except Exception as e:
    print(f"An error occurred: {e}")
    return None

#to extract content from url
import requests

def fetch_html_content(url):
  try:
    data=requests.get(url)
    return data.content
  except Exception as e:
        print('There was an error in fetching {}: {}'.format(url, e))
        return None

#to save extracted html and text content into file
import os
from os.path import exists
from bs4 import BeautifulSoup

def save_to_file(text,path,name):
  try:
    #check if file already exsists
    if exists(name):
          print('HTML file already exists')
          return


      #fun=open(os.path.join(path,name),"w")
    with open(os.path.join(path,name),"w") as file:
            file.write(str(text))
  except Exception as e:
        print(f'An error occurred while saving to file {name}: {e}')

from bs4 import BeautifulSoup
import requests
import mimetypes

from os.path import exists


def crawl_url(url, folder_path):

    filename=url.replace("/","_")
    try:
# url is pdf
      if filename.endswith(".pdf"):
        #checking if it already exsists
        if exists(filename):
          print('file already exists')
          return
        #downloading pdf
        response = requests.get(link)
        pdf = open(os.path.join(folder_path,filename), 'wb')
        pdf.write(response.content)
        pdf.close()

#other urls
      else:
        if exists(filename):
          print('file already exists')
          return
        # Fetch the website content using requests or another library.
        html_content = fetch_html_content(url)
        soup=BeautifulSoup(html_content,"html.parser")

        # Clean the HTML content using the clean_html function.
        extracted_text = clean_html_and_extract_text(html_content)
        # Store the HTML content and extracted text in separate files within the folder.
        # here filename is the crawled_url_in_underscores


        save_to_file(soup.prettify(), folder_path, f"{filename}_html.html")
        save_to_file(extracted_text, folder_path, f"{filename}_extracted_text.txt")
    except Exception as e:
      print(url,e)

#main function getting sitemap_path
import os
import requests

folder_path="/content/task1"
sitemap_path="https://raw.githubusercontent.com/Acuration/acuration-data-store/main/honeywell_sitemap.xml"
if not os.path.exists(folder_path):
    os.makedirs(folder_path)
    #to get list of links in siitemap
urls_list=parse_sitemap(sitemap_path)

for link in urls_list:
    #cleaning and extracting data
    crawl_url(link,"/content/task1")

from bs4 import BeautifulSoup
import requests
from os.path import exists

link="https://honeywell.com/content/dam/honeywellbt/en/documents/downloads/Honeywell-Hungary-%C3%89ves-energetikai-szakreferensi-riport-2020-Magyar.pdf"
response = requests.get(link)
pdf = open(os.path.join("/content/demo","name"), 'wb')
pdf.write(response.content)
pdf.close()



